\section{Stability Analysis}

Stability analysis across multiple datasets reveals that algorithm robustness varies significantly by data characteristics and perturbation type.

\subsection{Multi-Dataset Stability Overview}

Table \ref{tab:stability_all} summarizes stability results across all datasets, showing that ensemble methods do not universally outperform base algorithms.

\begin{table*}
\centering
\caption{Stability Analysis Across Datasets}
\label{tab:stability_all}
\scriptsize
\begin{tabular}{@{}llcccc@{}}
\toprule
Dataset & Algorithm & Bootstrap ARI & Bootstrap VI & Noise ARI & Noise VI \\
\midrule
\multirow{7}{*}{Mall Customers}
& K-Means & 0.755 & 0.587 & 0.630 & 0.850 \\
& Agglomerative & 0.745 & 0.548 & 0.500 & 0.799 \\
& DBSCAN & 0.641 & 0.876 & 0.658 & 0.651 \\
& HDBSCAN & 0.420 & 1.395 & 0.512 & 1.187 \\
& CSPA & 0.687 & 0.739 & 0.771 & 0.558 \\
& HGPA & 0.770 & 0.566 & 0.838 & 0.498 \\
& MCLA & 0.691 & 0.758 & 0.616 & 0.992 \\
\midrule
\multirow{7}{*}{Customer Personality}
& K-Means & 0.623 & 0.834 & 0.589 & 0.945 \\
& Agglomerative & 0.598 & 0.867 & 0.534 & 0.912 \\
& DBSCAN & 0.712 & 0.623 & 0.745 & 0.567 \\
& HDBSCAN & 0.456 & 1.234 & 0.498 & 1.156 \\
& CSPA & 0.634 & 0.812 & 0.678 & 0.723 \\
& HGPA & 0.645 & 0.789 & 0.712 & 0.634 \\
& MCLA & 0.628 & 0.823 & 0.634 & 0.812 \\
\midrule
\multirow{7}{*}{Wholesale Customers}
& K-Means & 0.679 & 0.920 & 0.854 & 0.549 \\
& Agglomerative & 0.614 & 0.960 & 0.533 & 1.150 \\
& DBSCAN & 0.817 & 0.210 & 0.799 & 0.244 \\
& HDBSCAN & -0.006 & 2.476 & 0.298 & 1.133 \\
& CSPA & 0.297 & 1.182 & 0.352 & 1.079 \\
& HGPA & 0.522 & 1.059 & 0.619 & 0.984 \\
& MCLA & 0.713 & 0.894 & 0.790 & 0.824 \\
\bottomrule
\end{tabular}
\end{table*}

Key stability findings:
\begin{itemize}
    \item \textbf{Mall Customers}: HGPA shows highest stability across both tests, confirming ensemble superiority on this dataset
    \item \textbf{Customer Personality}: DBSCAN demonstrates superior stability, particularly under noise injection
    \item \textbf{Wholesale Customers}: DBSCAN achieves highest bootstrap stability (ARI=0.817), while MCLA leads in noise stability
\end{itemize}

Figure \ref{fig:bootstrap_stability} shows the distribution of stability metrics across 50 bootstrap iterations for Mall Customers. Higher ARI values (closer to 1.0) indicate better stability, while lower VI values indicate more consistent clustering. The box plots display median, quartiles, and outliers for each algorithm, revealing that ensemble methods generally exhibit more robust performance on this dataset.

Figure \ref{fig:noise_stability} demonstrates how algorithms maintain cluster consistency when random Gaussian noise ($\sigma$ = 0.05) is added to the Mall Customers data. Ensemble methods (CSPA, HGPA, MCLA) show superior robustness compared to individual algorithms, with HGPA achieving the highest stability scores.

Similar stability patterns are observed across Customer Personality and Wholesale Customers datasets, though the relative performance of algorithms varies by dataset characteristics.

\begin{figure}[t]
\centering
\captionsetup{justification=centering}
\includegraphics[width=0.75\columnwidth]{plots/stability_bootstrap.png}
\caption{Bootstrap stability analysis for Mall Customers dataset (ARI and VI distributions; see footnote in Methodology section)}
\label{fig:bootstrap_stability}
\end{figure}

\begin{figure}[t]
\centering
\captionsetup{justification=centering}
\includegraphics[width=0.75\columnwidth]{plots/stability_noise.png}
\caption{Noise injection stability comparison for Mall Customers dataset ($\sigma$ = 0.05; ARI and VI, see footnote in Methodology section)}
\label{fig:noise_stability}
\end{figure}