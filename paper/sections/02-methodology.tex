\section{Methodology}

In this study, we employ a comprehensive approach to evaluate clustering algorithms across three real-world customer datasets, comparing four base methods with three ensemble techniques. The analysis focuses on customer segmentation using standardized features after robust preprocessing including outlier removal and feature transformation. This multi-dataset validation ensures findings are not dataset-specific and provides insights into algorithm performance across different data characteristics.

\subsection{Clustering Algorithms}

\subsubsection{Base Algorithms}

Our base clustering algorithms provide foundational methods for comparison. K-Means clustering partitions the data into k=5 clusters by minimizing the within-cluster sum of squares, using random initialization with 10 random starts to ensure robust results. Agglomerative clustering builds a hierarchy of clusters using Ward's linkage criterion, which minimizes the increase in within-cluster variance at each step. DBSCAN identifies dense regions of data points as clusters, with parameters tuned through k-distance analysis and a minimum of 5 samples per cluster. HDBSCAN extends DBSCAN by creating a hierarchical clustering structure, offering more flexible density-based clustering.

\subsubsection{Ensemble Methods}

To enhance clustering robustness, we implement three ensemble methods that combine multiple base clusterings. The Cluster-based Similarity Partitioning Algorithm (CSPA) constructs a similarity matrix based on the co-occurrence of data points in base clusters, then applies spectral clustering to this matrix for final partitioning. The HyperGraph Partitioning Algorithm (HGPA) represents clusters as hyperedges in a hypergraph and employs graph partitioning heuristics to determine the optimal clustering assignment. The Meta-CLustering Algorithm (MCLA) operates at the cluster level, computing similarities between different base clustering results and applying meta-clustering to produce the final ensemble solution.

\subsection{Validation Metrics}

We evaluate clustering quality using four complementary validation metrics. The Silhouette Score measures how well each point fits within its cluster compared to other clusters, with values ranging from -1 to 1, where higher scores indicate better clustering. The Davies-Bouldin Index computes the average similarity between each cluster and its most similar cluster, with lower values signifying better separation. The Calinski-Harabasz Index assesses the ratio of between-cluster dispersion to within-cluster dispersion, favoring higher values for well-separated clusters. Finally, Density-Based Clustering Validation (DBCV) provides a density-aware measure specifically designed for algorithms like DBSCAN and HDBSCAN.

\subsection{Stability Analysis}

\subsubsection{Bootstrap Stability}

To assess the robustness of clustering solutions under data resampling, we implement bootstrap stability analysis. This involves generating 50 bootstrap samples by sampling with replacement from the original dataset, applying each clustering algorithm to these samples, and comparing the results using Adjusted Rand Index (ARI) and Variation of Information (VI) metrics. The mean stability scores across all bootstrap iterations provide a comprehensive measure of algorithm consistency.

\subsubsection{Noise Injection Stability}

We further evaluate clustering robustness by introducing controlled perturbations. Gaussian noise with standard deviation $\sigma=0.05$ is added to the standardized data, and clustering algorithms are reapplied to the noisy dataset. Agreement between original and perturbed clustering results is measured using ARI and VI, revealing how well each method handles data uncertainty and noise.

\subsection{Implementation Details}

The complete analysis is implemented in Python, leveraging scikit-learn for base algorithms, hdbscan for density-based clustering, and custom implementations for ensemble methods. All experiments use fixed random seeds to ensure reproducibility. The codebase is organized modularly, with separate components handling data preprocessing, algorithm execution, evaluation metrics, and result visualization.